# Conditional Variational AutoEncoder (`Cond_VAE`) - Example using `MNIST` dataset

Conditional Variational AutoEncoder (`Cond_VAE`) is a generative model which assumes that the data is generated by some random process, involving unobserved continuous random variables. Like any other AutoEncoder, it has an `encoder` and a `decoder`.

The __encoder__ tries to learn q(z|x, y) which is the same as learning the hidden representation of the data x, conditioned on y (in this case, the labels of `MNIST` images). The __decoder__ on the other hand tries to learn p(x|z, y) which is just decoding the hidden representation to input space conditioned by y.

For this example, MNIST dataset has been used. Here are some of the reconstructed samples produced from randomly taken images from the `MNIST` _test set_:

![Model_prediction.jpg](https://github.com/randomaccess2023/MG2023/blob/main/Video%2051/Model_prediction.jpg "Model_prediction.jpg")

I also tried to generate desired images (according to the labels) from __random noise__ and they look like this:

![Generated_images_from_noise.jpg](https://github.com/randomaccess2023/MG2023/blob/main/Video%2051/Generated_images_from_noise.jpg "Generated_images_from_noise.jpg")
